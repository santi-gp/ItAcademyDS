{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bee2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup, Doctype\n",
    "from bs4 import Comment\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9cda9",
   "metadata": {},
   "source": [
    "## Descargar nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "still-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pages(num):\n",
    "    url=f'https://jobcube.com/Data-Science-jobs?pg={num}'\n",
    "    response=requests.get(url)\n",
    "    page = BeautifulSoup(response.text, 'lxml')\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noble-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_page = [i for i in range(1,61)]\n",
    "link_page=[all_pages(i) for i in id_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convenient-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "for i in range(len(link_page)):\n",
    "    links = link_page[i].find_all('h2','p-job_title')\n",
    "    for i in links:\n",
    "        children = i.find(\"a\",'s-biggerLink')\n",
    "        urls.append(children.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "verified-marketplace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "immune-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "    }\n",
    "\n",
    "def links_htmls(k):\n",
    "    r = requests.get(urls[k], headers=HEADERS)\n",
    "    soup=BeautifulSoup(r.text,'lxml')\n",
    "    \n",
    "    [x.extract() for x in soup.find_all('svg')]\n",
    "    [x.extract() for x in soup.find_all('img')]\n",
    "\n",
    "    title = soup.find(\"title\")\n",
    "    p = soup.find(\"p\",class_='p-detail_head_title')\n",
    "    description = soup.find(\"div\",class_='p-detail_content')\n",
    "\n",
    "    head = f\"<head>{title}</head>\"\n",
    "    h1=f'<h1>{p.get_text()}</h1>'\n",
    "    body = f\"<body>{h1}{description}</body>\"\n",
    "    html_output = f\"<html>{head}{body}</html>\"\n",
    "\n",
    "    html_output= re.sub(r'[\\t\\r\\n]', '', html_output)\n",
    "\n",
    "    with open(f'/media/richi/1C0C-8070/ItAcademyDS/sprint12/jobs_usa/jobcube{k}.html', 'w',encoding='utf-8') as f:\n",
    "        f.write(str(html_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "signal-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(urls)):\n",
    "    links_htmls(i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-justice",
   "metadata": {},
   "source": [
    "```python\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "##### Web scrapper for infinite scrolling page #####\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://jooble.org/SearchResult?rgns=United%20States&ukw=data%20science\")\n",
    "time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    # scroll one screen height each time\n",
    "    driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "    i += 1\n",
    "    time.sleep(scroll_pause_time)\n",
    "    # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "    scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "    # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "    if (screen_height) * i > scroll_height:\n",
    "        break \n",
    "\n",
    "##### Extract Reddit URLs #####\n",
    "link = driver.find_elements_by_xpath('//a[@data-testid=\"jobTitle\"]')\n",
    "links = [elem.get_attribute('href') for elem in link]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-mailman",
   "metadata": {},
   "source": [
    "```python\n",
    "r = requests.get(urls[0], headers=HEADERS)\n",
    "soup=BeautifulSoup(r.text,'lxml')\n",
    "    \n",
    "[x.extract() for x in soup.find_all('svg')]\n",
    "[x.extract() for x in soup.find_all('img')]\n",
    "\n",
    "\n",
    "title = soup.find(\"title\")\n",
    "    \n",
    "p = soup.find(\"p\",class_='p-detail_head_title')\n",
    "    \n",
    "description = soup.find(\"div\",class_='p-detail_content')\n",
    "    \n",
    "head = f\"<head>{title}</head>\"\n",
    "h1=f'<h1>{p.get_text()}</h1>'\n",
    "body = f\"<body>{h1}{description}</body>\"\n",
    "html_output = f\"<html>{head}{body}</html>\"\n",
    "\n",
    "html_output= re.sub(r'[\\t\\r\\n]', '', html_output)\n",
    "\n",
    "with open('jobcube.html', 'w',encoding='utf-8') as f:\n",
    "    f.write(str(html_output))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-wagner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
